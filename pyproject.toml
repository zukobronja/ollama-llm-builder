[project]
name = "ollama-llm-builder"
version = "0.1.0"
description = "Build and deploy LLMs to Ollama that are not currently available in the official model library"
readme = "README.md"
requires-python = ">=3.9"
authors = [
    { name = "Zuko" }
]

dependencies = [
    # Core dependencies
    "torch>=2.0.0",
    "transformers>=4.35.0",
    "huggingface-hub>=0.19.0",
    "pyyaml>=6.0",

    # Image processing
    "pillow>=10.0.0",

    # Utilities
    "tqdm>=4.65.0",
    "requests>=2.31.0",
    "python-dotenv>=1.0.0",

    # Performance
    "accelerate>=0.24.0",
    "safetensors>=0.4.0",

    # GGUF conversion dependencies (for llama.cpp)
    "numpy>=1.24.0",
    "sentencepiece>=0.1.99",
    "gguf>=0.1.0",
    "mistral-common>=1.0.0",
    "protobuf>=3.20.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
]

[project.scripts]
ollama-build = "build:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["scripts"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]

[tool.black]
line-length = 100
target-version = ['py38']

[tool.ruff]
line-length = 100
target-version = "py38"

[tool.ruff.lint]
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "B",   # flake8-bugbear
    "C4",  # flake8-comprehensions
]
ignore = [
    "E501",  # line too long (handled by black)
]
